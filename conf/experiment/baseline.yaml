# @package _global_

experiment:
  name: baseline
  params:
    epoch_count: 2
    batch_size: 2

pipeline:
  optimizer:
    learning_rate: 6.25e-5
  tokenizer:
    name: GPT2Tokenizer
    special_tokens:
      pad_token: "[PAD]"
      sep_token: "[SEP]"
      cls_token: "[CLS]"
      mask_token: "[MASK]"
    truncation: true
    padding: true
    max_length: 512
  model:
    name: GPT2Model
