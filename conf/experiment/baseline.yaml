# @package _global_

experiment:
  name: baseline

params:
  epoch_count: 5
  lr: 1e-5
  batch_size: 32

pipeline:
  tokenizer:
    name: GPT2Tokenizer
    special_tokens:
      pad_token: "[PAD]"
      sep_token: "[SEP]"
      cls_token: "[CLS]"
      mask_token: "[MASK]"
    truncation: true
    padding: true
    max_length: 512
  model:
    name: GPT2Model
